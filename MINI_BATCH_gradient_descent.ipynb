{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNigVJDBpvv10PZVo66pW5q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smritiguleria/demo_wscubepython/blob/main/MINI_BATCH_gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import time\n",
        "import datetime\n",
        "import os"
      ],
      "metadata": {
        "id": "2kFzhfpjNL0s"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your neural network architecture with batch normalization\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Flatten(),                   # Flatten the input image tensor\n",
        "            nn.Linear(28 * 28, 64),         # Fully connected layer from 28*28 to 64 neurons\n",
        "            nn.BatchNorm1d(64),             # Batch normalization for stability and faster convergence\n",
        "            nn.ReLU(),                      # ReLU activation function\n",
        "            nn.Linear(64, 32),              # Fully connected layer from 64 to 32 neurons\n",
        "            nn.BatchNorm1d(32),             # Batch normalization for stability and faster convergence\n",
        "            nn.ReLU(),                      # ReLU activation function\n",
        "            nn.Linear(32, 10)               # Fully connected layer from 32 to 10 neurons (for MNIST classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "imnT19frNSex"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(47)\n",
        "\n",
        "    # Load the MNIST dataset\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    train_data = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())\n",
        "    train_loader = DataLoader(train_data, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUtQcPp5NYVa",
        "outputId": "acd62418-4b02-4524-a604-6c5a8430d6c7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /content/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 115354147.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST/raw/train-images-idx3-ubyte.gz to /content/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /content/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 30489729.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST/raw/train-labels-idx1-ubyte.gz to /content/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /content/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 28039064.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST/raw/t10k-images-idx3-ubyte.gz to /content/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /content/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 14238063.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST/raw/t10k-labels-idx1-ubyte.gz to /content/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = MLP() # Initialize MLP model\n",
        "loss_function = nn.CrossEntropyLoss()    # Cross-entropy loss function for classification\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-3)   # Adam optimizer with learning rate 0.001"
      ],
      "metadata": {
        "id": "SdlQHjGTNfn8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(3):   # Iterate over 3 epochs\n",
        "    print(f'Starting epoch {epoch + 1}')\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()   # Zero the gradients\n",
        "        outputs = mlp(inputs.view(inputs.shape[0], -1))   # Flatten the input for MLP and forward pass\n",
        "        loss = loss_function(outputs, labels)   # Compute the loss\n",
        "        loss.backward()   # Backpropagation\n",
        "        optimizer.step()   # Optimizer step to update parameters\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:   # Print every 100 mini-batches\n",
        "            print(f'Epoch {epoch + 1}, Mini-batch {i + 1}, Loss: {running_loss / 100}')\n",
        "            running_loss = 0.0\n",
        "print('Training finished')\n",
        "\n",
        "end_time = time.time() # Record end time\n",
        "print('Training process has been completed. ')\n",
        "training_time = end_time - start_time\n",
        "\n",
        "print('Training time:', str(datetime.timedelta(seconds=training_time))) # for calculating the training time in minutes and seconds format"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3GlxrYBNqSH",
        "outputId": "4a455cc9-f043-4779-f80d-7744f8947288"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n",
            "Epoch 1, Mini-batch 100, Loss: 1.1071095156669617\n",
            "Epoch 1, Mini-batch 200, Loss: 0.48408970385789873\n",
            "Epoch 1, Mini-batch 300, Loss: 0.3104418051242828\n",
            "Epoch 1, Mini-batch 400, Loss: 0.26336906000971794\n",
            "Epoch 1, Mini-batch 500, Loss: 0.22288601607084274\n",
            "Epoch 1, Mini-batch 600, Loss: 0.20098184302449226\n",
            "Epoch 1, Mini-batch 700, Loss: 0.18423103533685206\n",
            "Epoch 1, Mini-batch 800, Loss: 0.16405216380953788\n",
            "Epoch 1, Mini-batch 900, Loss: 0.14631170395761728\n",
            "Starting epoch 2\n",
            "Epoch 2, Mini-batch 100, Loss: 0.12182405546307563\n",
            "Epoch 2, Mini-batch 200, Loss: 0.1157226226851344\n",
            "Epoch 2, Mini-batch 300, Loss: 0.12297858588397503\n",
            "Epoch 2, Mini-batch 400, Loss: 0.12761864580214025\n",
            "Epoch 2, Mini-batch 500, Loss: 0.12739025708287954\n",
            "Epoch 2, Mini-batch 600, Loss: 0.10691166184842586\n",
            "Epoch 2, Mini-batch 700, Loss: 0.12470327839255332\n",
            "Epoch 2, Mini-batch 800, Loss: 0.10864155779592694\n",
            "Epoch 2, Mini-batch 900, Loss: 0.10706586236134172\n",
            "Starting epoch 3\n",
            "Epoch 3, Mini-batch 100, Loss: 0.09460175671614707\n",
            "Epoch 3, Mini-batch 200, Loss: 0.08580301646143199\n",
            "Epoch 3, Mini-batch 300, Loss: 0.08968512518331409\n",
            "Epoch 3, Mini-batch 400, Loss: 0.08367499428801238\n",
            "Epoch 3, Mini-batch 500, Loss: 0.08743745630607008\n",
            "Epoch 3, Mini-batch 600, Loss: 0.0885771809425205\n",
            "Epoch 3, Mini-batch 700, Loss: 0.08851641036570072\n",
            "Epoch 3, Mini-batch 800, Loss: 0.08432795900851488\n",
            "Epoch 3, Mini-batch 900, Loss: 0.09155286164954304\n",
            "Training finished\n",
            "Training process has been completed. \n",
            "Training time: 0:00:36.660559\n"
          ]
        }
      ]
    }
  ]
}